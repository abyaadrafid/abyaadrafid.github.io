<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.6.3">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Rafid Abyaad">

  
  
  
    
  
  <meta name="description" content="In this post, we will be looking at the paper PnPNet: End-to-End Perception and Prediction with Tracking in the Loop, by Liang et al., which was published in CVPR 2020 [1]. After defining our task and discussing some related research in this field, we will be looking at the methodology of the paper. Then we will analyze the quantitative results and have a look at the qualitative results. Finally, we will finish it off with some remarks and possible ideas for extension.">

  
  <link rel="alternate" hreflang="en-us" href="https://abyaadrafid.github.io/post/pnpnet/">

  


  
  
  
  <meta name="theme-color" content="rgb(0, 136, 204)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://abyaadrafid.github.io/post/pnpnet/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Homepage | Rafid">
  <meta property="og:url" content="https://abyaadrafid.github.io/post/pnpnet/">
  <meta property="og:title" content="PnPNet: End-to-End Perception and Prediction With Tracking in the Loop | Homepage | Rafid">
  <meta property="og:description" content="In this post, we will be looking at the paper PnPNet: End-to-End Perception and Prediction with Tracking in the Loop, by Liang et al., which was published in CVPR 2020 [1]. After defining our task and discussing some related research in this field, we will be looking at the methodology of the paper. Then we will analyze the quantitative results and have a look at the qualitative results. Finally, we will finish it off with some remarks and possible ideas for extension."><meta property="og:image" content="https://abyaadrafid.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://abyaadrafid.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2021-04-25T19:36:52&#43;06:00">
    
    <meta property="article:modified_time" content="2021-04-25T19:36:52&#43;06:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://abyaadrafid.github.io/post/pnpnet/"
  },
  "headline": "PnPNet: End-to-End Perception and Prediction With Tracking in the Loop",
  
  "datePublished": "2021-04-25T19:36:52+06:00",
  "dateModified": "2021-04-25T19:36:52+06:00",
  
  "author": {
    "@type": "Person",
    "name": "Rafid Abyaad"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Homepage | Rafid",
    "logo": {
      "@type": "ImageObject",
      "url": "https://abyaadrafid.github.io/img/icon-512.png"
    }
  },
  "description": "In this post, we will be looking at the paper PnPNet: End-to-End Perception and Prediction with Tracking in the Loop, by Liang et al., which was published in CVPR 2020 [1]. After defining our task and discussing some related research in this field, we will be looking at the methodology of the paper. Then we will analyze the quantitative results and have a look at the qualitative results. Finally, we will finish it off with some remarks and possible ideas for extension."
}
</script>

  

  


  


  





  <title>PnPNet: End-to-End Perception and Prediction With Tracking in the Loop | Homepage | Rafid</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  

<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Homepage | Rafid</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Homepage | Rafid</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#skills"><span>Skills</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#experience"><span>Experience</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/Abyaad_CV.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>PnPNet: End-to-End Perception and Prediction With Tracking in the Loop</h1>

  
  <p class="page-subtitle">A review of a CVPR paper in the paradigm of autonomous driving</p>
  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 25, 2021
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    17 min read
  </span>
  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>In this post, we will be looking at the paper <a href="https://arxiv.org/abs/2005.14711">PnPNet: End-to-End Perception and Prediction with Tracking in the Loop</a>, by <em>Liang et al.</em>,  which was published in CVPR 2020 <a href="#1">[1]</a>. After defining our task and discussing some related research in this field, we will be looking at the methodology of the paper. Then we will analyze the quantitative results and have a look at the qualitative results. Finally, we will finish it off with some remarks and possible ideas for extension.</p>
<h1 id="introduction">Introduction</h1>
<p>In the context of self-driving vehicles, predicting the motion of other vehicles is a critical task. Approximating the trajectory of neighboring agents in the future is equally as important as detecting them in the current time frame. To do this task, so far three paradigms have been proposed.</p>
<p>The first divide this problem into three separate sub-tasks, which are handled by completely independent sub-systems. These three tasks, namely, object detection, object tracking, and motion forecasting are done sequentially. As they are developed separately, they need more computing power and cannot correct mistakes from upstream tasks. The second paradigm tries to solve the detection and prediction task with a single neural network. This yields more efficient computation but these models suffer from limited use of temporal history and are vulnerable to occlusion.</p>
<p><img src="autonomy_stacks.png" alt="image"></p>
<figcaption>
<p>Three paradigms of perception and prediction problems <a href="#1">[1]</a></p>
</figcaption>
<p>This paper introduces the new third paradigm. It argues that, for sequential modeling tasks such as motion forecasting, past data is very important. To that end, it proposes PnPNet which combines multi-object tracking with joint perception and prediction models. We will go into the details of the model after discussing some other works that try to tackle different aspects of our problem.</p>
<h1 id="related-works">Related works</h1>
<h3 id="3d-object-detection">3D object detection</h3>
<p>The use of depth sensors such as LiDARs has been shown to have better performance <a href="#2">[2]</a> than cameras for 3D detection. Some works also explore a fusion of LiDAR point clouds and camera inputs <a href="#3">[3]</a>.
<img src="fusion_model.png" alt="image"></p>
<figcaption>
<p>A qualitative result of fusion models <a href="#3">[3]</a></p>
</figcaption>
<h3 id="multi-object-tracking">Multi-Object Tracking</h3>
<p>Multi-Object tracking is a system to track multiple objects at the same time. It consists of a discrete data association problem and a continuous trajectory estimation problem <a href="#4">[4]</a>. There have been efforts to handle occlusion with hand-crafted heuristics <a href="#5">[5]</a> and single object tracking <a href="#6">[6]</a> to handle occlusion. To handle the trajectory problem, some approaches also use sensor features<a href="#7">[7]</a>.</p>
<p><img src="multi-object_tracking.png" alt="image"></p>
<figcaption>
<p>An example of multi-object tracking takes from <a href="#8">[8]</a></p>
</figcaption>
<h3 id="motion-forecasting">Motion Forecasting</h3>
<p>Different methods try to approach the multi-agent motion forecasting problem. <em>Alahi et al.</em> propose LSTM based social pooling to model motion <a href="#9">[9]</a>. Social-GAN <a href="#10">[10]</a> improves on it using adversarial training. The use of sensor features is also explored, but these methods usually have generalization issues when applied to noisy data <a href="#11">[11]</a>. Raster representation in BEV that encodes both perception and map information is widely used as well <a href="#2">[2]</a>.</p>
<h3 id="joint-models-for-perception-and-prediction">Joint models for Perception and Prediction</h3>
<p>The FAF paper <a href="#12">[12]</a> of <em>Luo et al.</em> serves as a direct predecessor and an evaluation baseline for PnPNet. This model uses a single convolutional backbone to detect and predict future motion. NeuralMP <a href="#13">[13]</a> shares motion planning features with perception and prediction to allow end-to-end training.</p>
<p><img src="neuralmp.png" alt="image"></p>
<figcaption>
<p>Some qualitative results from NeuralMP <a href="#13">[13]</a></p>
</figcaption>
<h3 id="motivations-for-the-approach">Motivations for the approach</h3>
<p>Research efforts in this sector have developed the field of autonomous driving a lot. However, they also have some shortcomings. Multi-object tracking with supplementary sensor readings can not effectively use a long history of temporal data. These models saturate after three seconds <a href="#7">[7]</a>. Also most-multi object tracking efforts only tackle one of the two tasks at a time.</p>
<p>Although earlier joint models for perception and prediction use sensor information, they do not exploit the salient features of actors in the time dimension. As a result, their overall performance might be vastly improved with some adjustments.</p>
<p>As we will see, PnPNet does exactly that. Where earlier models kept the tracking task at the end of the task stack, PnPNet pulls it inside the perception-prediction loop. By doing that, it can utilize trajectory-level representation and can improve performance in all related tasks. It also solves the discrete-continuous tracking problem that other works overlook with a novel tracking framework. PnPNet deviates from the common approach of <a href="#2">[2]</a> and directly uses perception features for better scene contexts.</p>
<h1 id="proposed-method-pnpnet">Proposed method: PnPNet</h1>
<h3 id="technical-contributions">Technical Contributions</h3>
<p>The discussed related research overlook an important aspect of our problem. They do not take the temporal characteristics of actors into account. To allow for that, this paper makes two major technical contributions :</p>
<ol>
<li>It introduces a new trajectory representation based on a sequence of detections through time.</li>
<li>It proposes a multi-object tracker that solves both the association and the trajectory estimation problem.</li>
</ol>
<h1 id="model-overview">Model Overview</h1>
<p>PnPNet consists of three separate modules :</p>
<ol>
<li>3D Object detection module</li>
<li>Discrete-Continuous Tracking module</li>
<li>Motion forecasting module</li>
</ol>
<p><img src="model.png" alt="image"></p>
<figcaption>
<p>A summary of PnPNet workflow <a href="#1">[1]</a></p>
</figcaption>
<h2 id="3d-object-detection-module">3D Object detection module</h2>
<p>The detection module takes multi-sweep LiDAR point cloud representation in birds-eye-view and an HD map as input. Optionally, geometric and semantic information of the HD map can also be used. 2D convolutional neural network-based backbone with multi-scale feature fusion is applied to the input, which generates the intermediate BEV features. A convolutional detection header is then used on the intermediate features to create dense object detections at each time step. We maintain a database of intermediate features at each time step because they will be needed to in the tracking module downstream.</p>
<p><img src="3dmodule.png" alt="image"></p>
<figcaption>
<p>Workflow of the 3D detection module</p>
</figcaption>
<h2 id="discrete-continuous-tracking-module">Discrete-Continuous Tracking module</h2>
<p>As discussed in the related works, there are two separate challenges in multi-object tracking. Previous works mostly focus on the discrete problem of data association but PnPNet also takes the continuous problem of trajectory estimation into account. The paper argues that it helps to prevent association errors from accumulating over time and reduces the variance in motion history. To that end, it proposes a two-stage tracking framework. To use the framework, rich and concise trajectory level object representation needs to be learned.</p>
<p><img src="trackingrep.png" alt="image"></p>
<figcaption>
<p>Trajectory level representation generation</p>
</figcaption>
<p>The representation learning problem is formulated as a sequence modeling problem. We use the intermediate FEV features from the backbone network and the location information of each object at time t to run a Bilinear Interpolation. Note that the location information is obtained from a database of older trajectories. This output is regarded as the representation of the motion of each object from the start frame to the current frame. Along with this, the absolute velocities of each object are passed into an MLP/feed-forward network.</p>
<p>The merged features from this FF network are then used as the sequential input for our LSTM. This LSTM hidden state is our trajectory level representation at each time step.</p>
<h3 id="data-association">Data Association</h3>
<p>The first stage of the tracking framework is the discrete tracker. We obtain the dense detections from the 3D detection module upstream and the trajectories from the database of older tracks. At time t, given N<sub>t</sub> detections and M<sub>t-1</sub> trajectories, it tries to determine the associations between them. This association problem is formulated as a bipartite graph matching problem. As a result, one-to-one matching is guaranteed. The edge values for our bipartite matching problem are the affinity values, which represent how likely is a track M<sub>t-1</sub> to belong to detection N<sub>t</sub>. The affinity matrix :
$$
C_{i,j} =
\begin{cases}
MLP_{pair}(f(D_{i}^t); h(P_{j}^{t−1})) &amp; if\ 1 ≤ j ≤ M_{t−1}\\<br>
MLP_{unary}(f(D_{i}^t)) &amp; if\ j = M_{t−1} + i \\<br>
−inf &amp; otherwise \\<br>
\end{cases}    <br>
$$
The affinity values are calculated with binary or unary MLPs. If we have more or equal number of tracks at the previous step than detections at this step, we use the binary MLP. Otherwise, the unary MLP is used. This bipartite system is solved optimally using the Hungarian algorithm. We use single object tracking for older unmatched tracks. Combining results from bipartite matching and SOT yields a final set of unrefined tracks P<sub>t</sub>. This set of unrefined tasks have the candidates for updating the database of older tracks to be used in the future time steps.</p>
<p><img src="association.png" alt="image"></p>
<figcaption>
<p>Associating tracks with detections</p>
</figcaption>
<h4 id="single-object-tracking">Single object tracking</h4>
<p>The single object tracker used in the PnPNet paper follows nearly the same methodology as the Siamese tracker <a href="#14">[14]</a>. The Siamese tracker has two networks. One of these twin networks receives an exemplar image as input, another one receives a search image as input. The task for the twin network is to try and find the exemplar image within the search image.</p>
<p><img src="sot.png" alt="image"></p>
<figcaption>
<p>Siamese tracker from <a href="#14">[14]</a>.</p>
</figcaption>
<p>Siamese trackers usually have a cross-correlation layer at the end. PnPNet chooses to replace this layer with an MLP with learnable parameters.</p>
<h4 id="hungarian-algorithm">Hungarian Algorithm</h4>
<p>The Hungarian algorithm is an optimization algorithm that produces the best one-to-one matching when applied to a bipartite graph. In our context, two sets of nodes are the detections and the tracks, denoted as <strong>N</strong> and <strong>M</strong> respectively. The edges between these sets of nodes are the affinity values, denoted by <strong>a</strong>. Here we see an example formulation :</p>
<pre><code class="language-mermaid">graph TD;
id1((&quot;N&lt;sub&gt;1&lt;/sub&gt;&quot;))-- &quot;a&lt;sub&gt;1&lt;/sub&gt;&quot; ---id2((&quot;M&lt;sub&gt;1&lt;/sub&gt;&quot;));
id1((&quot;N&lt;sub&gt;1&lt;/sub&gt;&quot;))-- &quot;a&lt;sub&gt;4&lt;/sub&gt;&quot; ---id3((&quot;M&lt;sub&gt;2&lt;/sub&gt;&quot;));
id1((&quot;N&lt;sub&gt;1&lt;/sub&gt;&quot;))-- &quot;a&lt;sub&gt;7&lt;/sub&gt;&quot; ---id4((&quot;M&lt;sub&gt;3&lt;/sub&gt;&quot;));

id6((&quot;N&lt;sub&gt;2&lt;/sub&gt;&quot;))-- &quot;a&lt;sub&gt;2&lt;/sub&gt;&quot; ---id2((&quot;M&lt;sub&gt;1&lt;/sub&gt;&quot;));
id6((&quot;N&lt;sub&gt;2&lt;/sub&gt;&quot;))-- &quot;a&lt;sub&gt;5&lt;/sub&gt;&quot; ---id3((&quot;M&lt;sub&gt;2&lt;/sub&gt;&quot;));
id6((&quot;N&lt;sub&gt;2&lt;/sub&gt;&quot;))-- &quot;a&lt;sub&gt;8&lt;/sub&gt;&quot; ---id4((&quot;M&lt;sub&gt;3&lt;/sub&gt;&quot;));

id7((&quot;N&lt;sub&gt;3&lt;/sub&gt;&quot;))-- &quot;a&lt;sub&gt;3&lt;/sub&gt;&quot; ---id2((&quot;M&lt;sub&gt;1&lt;/sub&gt;&quot;));
id7((&quot;N&lt;sub&gt;3&lt;/sub&gt;&quot;))-- &quot;a&lt;sub&gt;6&lt;/sub&gt;&quot; ---id3((&quot;M&lt;sub&gt;2&lt;/sub&gt;&quot;));
id7((&quot;N&lt;sub&gt;3&lt;/sub&gt;&quot;))-- &quot;a&lt;sub&gt;9&lt;/sub&gt;&quot; ---id4((&quot;M&lt;sub&gt;3&lt;/sub&gt;&quot;));
</code></pre>
<p>Hungarian algorithm will find the best matching that maximizes the overall affinity value throughout the graph.</p>
<h3 id="trajectory-estimation">Trajectory Estimation</h3>
<p>Trajectory estimation is the second stage in our two-stage tracking network. This module re-evaluates each object track for the new observation at the current frame and helps minimize false positives and localization errors from upstream tasks. The LSTM representation is refined by an MLP according to the current association. The MLP outputs a confidence score and center position offset. 
$$
score_{i}, \Delta u_{i}^{t-T_0+1:t}, \Delta v_{i}^{t-T_0+1:t} = MLP_{refine}(h(P_{i}^t))
$$
Using these confidence scores, a non-maximal suppression is applied and only top M<sub>t</sub> out of P<sub>t</sub> tracks are accepted. Other tracks are discarded as false positives. The database of trajectories is then updated with this accepted tracks set.</p>
<p><img src="trajectory.png" alt="image"></p>
<figcaption>
<p>Trajectory estimation sub-module.</p>
</figcaption>
<h2 id="motion-forecasting-module">Motion Forecasting module</h2>
<p>Earlier joint perception and prediction models design the prediction module as another convolutional header. However, PnPnet puts tracking on the detection backbone and creates object trajectory representation as LSTM hidden states. These hidden states are then passed through an MLP to estimate motion prediction for each object we are tracking in each time step. 
$$
\Delta u_{i}^{t:t+\Delta T}, \Delta v_{i}^{t:t+\Delta T} = MLP_{refine}(h(P_{i}^t))
$$
Here $\Delta T$ is a parameter of the model that denotes the prediction horizon.</p>
<h2 id="end-to-end-training">End-to-End Training</h2>
<p>As the model has a lot of moving parts, it is hard to visualize how the individual modules interact with each other. So here is an illustration that summarizes the workflow:
<img src="e2etrain.jpeg" alt="image"></p>
<figcaption>
<p>Each rectangular box represents a major process. These boxes are color coded according to their individual modules. A database of tracks is represented with the cylindrical icon.</p>
</figcaption>
<p>As we can see from the figure, PnPNet only has one set of inputs and one set of outputs. This means it can be trained end-to-end.</p>
<p>To train PnPNet end-to-end, a multi-task loss is calculated from the individual losses of its three modules. 
$$
L = L_{detect}+ L_{track} + L_{predict}
$$
Here the detection loss is the cross-entropy loss and the smoothed l<sub>1</sub> loss over bounding boxes. The tracking loss is again another multi-task loss defined as : 
$$
L_{track} = L_{score}^{affinity} + L_{score}^{sot} + L_{score}^{refine} + L_{reg}^{refine}
$$
The max-margin loss is applied to the sot scores, the trajectory scores, and the affinity matrix.
$$
L_{score} = \frac{1}{N_{i,j}} \sum_{i \epsilon{pos}, j \epsilon{neg}} max(0, m − (a_i − a_j))
$$
Here N<sub>i,j</sub> is the total number of positive and negative pairs, where a<sub>i</sub> and a<sub>j</sub> are scores for positive and negative samples accordingly.</p>
<p>For the prediction task, the loss is just the smoothed l<sub>1</sub> loss of the predictions.</p>
<p>The Adam optimizer is used to train PnPNet with a frame rate of 10Hz. At each frame maximum M = 50 tracks and N = 50 detections are maintained. The prediction horizon is set $\Delta T$= 3 seconds with 0.5 seconds interval.</p>
<h1 id="datasets-and-metrics">Datasets and Metrics</h1>
<p>The PnPNet paper tests itself on two major driving datasets. The performance of PnPNet is measured from modular metrics of detection and tracking, as well as from system metrics for end-to-end perception and prediction.</p>
<h3 id="nuscenes-dataset-1515">nuScenes Dataset <a href="#15">[15]</a></h3>
<p>This dataset consists of 1000 20-seconds long log snippets with LiDAR sweeps and 3D labels for objects and corresponding HD maps. Although this dataset has some caveats because it only has 84 unique driving journeys, and 63.5% of the cars are parked.</p>
<p><img src="nuScenes.png" alt="image"></p>
<figcaption>
<p>An example from nuScences <a href="#15">[15]</a></p>
</figcaption>
<h3 id="atg4d-dataset-1616">ATG4D Dataset <a href="#16">[16]</a></h3>
<p>To simulate real-world driving scenario, the authors validate PnPNet on the ATG4D dataset. It contains over 5000 log snippets of 1000 unique journeys. Each snippet with LiDAR sweeps and HD maps. Also, only 48.1% of the cars are parked.</p>
<h3 id="modular-metrics">Modular Metrics</h3>
<p>For detection and tracking tasks, PnPNet follows tracking metrics introduced by nuScenes <a href="#15">[15]</a>. Detection task is measured in terms of average precision (AP) and tracking task in terms of MOT metrics, defined by <em>Bernardin et al.</em><a href="#17">[17]</a>.</p>
<h3 id="system-metrics">System Metrics</h3>
<p>Unlike predefined modular metrics, system metrics are defined by the PnPNet paper itself. For the perception task, average precision and maximum recall are used. For the prediction task, PnPNet uses average displacement error (ADE) and final displacement error (FDE). Both of these displacement errors are measured over 3 seconds, with ADE over a 0.5-second interval. Displacement error is simply the difference between the actual and the predicted location of an object.</p>
<h1 id="results">Results</h1>
<p>In this section, we look at the quantitative results, some ablation studies, and some qualitative results on PnPnet.</p>
<h2 id="quantitative-results">Quantitative results</h2>
<h3 id="detection-and-tracking">Detection and tracking</h3>
<p>The detection module is evaluated on nuScenes in comparison to other SOA detections. PnPnet outperforms leading model <em>Megvii</em><a href="#18">[18]</a> in most metrics. The tracking module is evaluated against the leading approach <em>StanfordIPRL-TRI</em><a href="#19">[19]</a> and a PnPNet baseline with KF tracker replacing the tracking module. PnPnet outperforms <a href="#19">[19]</a> and KF tracker in MOT metrics.</p>
<p><img src="tracking.png" alt="image"></p>
<figcaption>
<p>Evaluation of multi object tracking on nuScenes <a href="#1">[1]</a></p>
</figcaption>
<p>Even though there are not many significant changes in the object detection module, PnPNet being an end-to-end model benefits this task too. A Better prediction model also fine-tunes the detection module. Improvements in the tracking model then influence the prediction task.</p>
<h3 id="perception-and-prediction">Perception and prediction</h3>
<p>PnPnet is evaluated on both nuScenes and ATG4D for end-to-end perception and prediction. The paper establishes a baseline by removing the tracking module and denotes it as &ldquo;PnPnet w/o track&rdquo;. This implementation can be considered a re-implementation of <a href="#12">[12]</a>. The fast and furious (FAF)<a href="#12">[12]</a> is the direct predecessor of PnPNet. Classic PnPNet outperforms this baseline consistently across both datasets and all metrics, illustrating that keeping tracking in the loop allows for massive improvements in all tasks. 
<img src="e2eresult.png" alt="image"></p>
<figcaption>
<p>End-to-End results on ATG4D and nuScenes <a href="#1">[1]</a></p>
</figcaption>
<p>PnPNet observes up to 4.4 and 2.3 percentage increase in average precision and maximum recall respectively. For the prediction task, ADE is decreased up to 20% and FDE up to 15%.</p>
<h2 id="ablation-studies">Ablation studies</h2>
<p>Ablation studies are often conducted on end-to-end models to find out the individual contributions to the final result. PnPNet paper also conducts some ablation studies by removing motion features, sot, trajectory refinement, and the whole track module one byone.</p>
<p><img src="ablation.png" alt="image"></p>
<figcaption>
<p>Ablation studies on PnPNet <a href="#1">[1]</a></p>
</figcaption>
<p>From the results of the studies, it is obvious that the tracking module has the largest effect on the prediction results. ADE and FDE go up to 18% percent in the absence of the tracking module. The trajectory rescore module influences average precision and displacement errors. The single object tracking module has a decent contribution across all the metrics.</p>
<h2 id="qualitative-results">Qualitative Results</h2>
<p>Well, these explanations are good and all but, where are the tangible results? Let's look at the visualization of perception and prediction results on ATG4D. This demonstrates that by learning trajectory representations explicitly, PnPNet is able to handle occlusion and produces robust predictions.</p>












  


<video controls >
  <source src="qual.mp4" type="video/mp4">
</video>
<figcaption>
<p>Qualitative results of  <a href="#1">[1]</a></p>
</figcaption>
<p>This video was taken from the CVF youtube channel. The original video was the presentation for the PnPNet paper, which can be found <a href="https://www.youtube.com/watch?v=3dsXqhICdI8&amp;ab_channel=ComputerVisionFoundationVideos">here</a>.</p>
<h1 id="summary">Summary</h1>
<p>Earlier works in the realm of perception and prediction of autonomous driving have somewhat overlooked the contribution of trajectory information for future predictions. Although recent works do capitalize on the use of joint models, they leave tracking out of the loop. Following the footstep of these recent developments, PnPNet proposes a novel end-to-end training framework for perception and prediction while taking tracking into consideration. They also introduce a new tracking framework that can be looped into classic detection-prediction stacks. Their proposed improvements are then validated on different datasets and compared with other leading models in this paradigm. Finally, they show significant improvement in all three tasks as well as end-to-end performance.</p>
<h2 id="personal-remarks">Personal Remarks</h2>
<ul>
<li>In the paper, the authors assume the maximum number of detection and tracks to be 50. But, on busy roads usually there are far more moving objects within close proximity. The only way for PnPNet to consider this is to increase the <em>N</em> and <em>M</em> parameters. However, this again results in a massive number of computations for the Hungarian algorithm. The paper doesn't take this problem into account.</li>
<li>The authors only store tracks up to 16 older frames to maintain real-time performance. However, ablation studies show that, keeping tracks up to 32 frames achieves better prediction result. This points to the fact that this model has some performance-efficiency trade-off.</li>
<li>The predecessor of PnPNet, FaF<a href="#12">[12]</a> did not have an explicit tracking module, but still outperformed the Hungarian matching. PnPNet, however, incorporates the Hungarian algorithm in it's design and outperforms FaF. We can posit that continuous track refinement has helped in the association task as well.</li>
<li>PnPNet uses an LSTM unit in the tracking module. However, as we know LSTMs need quite a bit of computation resources. In future works, other less expensive recurrent units can be explored for the same task. For example, GRU units <a href="#20">[20]</a> are computationally less expensive and have been proven to be comparable in terms of performance <a href="#21">[21]</a>.</li>
<li>Recently many sequence modeling tasks are leveraging the attention mechanism. As we define our tracking as a sequential problem, it may be useful to apply self-attention on top of the recurrent unit outputs. Although this is not a pure sequence problem, self-attention may refine and provide importance to better trajectory features. Also, built-in attention <a href="#22">[22]</a> can be applied to the 2D-Convolutional backbone of the 3D detection algorithm.</li>
</ul>
<h1 id="references">References</h1>
<p id="1">
[1] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, Raquel Urtasun, "PnPNet: End-to-End Perception and Prediction with Tracking in the Loop", in CVPR, 2020.
</p>
<p id="2">
[2] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Realtime 3d object detection from point clouds. In CVPR, 2018.
</p>
<p id="3">
[3] Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun. Deep continuous fusion for multi-sensor 3d object detection. In ECCV, 2018.
</p>
<p id="4">
[4] Anton Milan, Konrad Schindler, and Stefan Roth. Multitarget tracking by discrete-continuous energy minimization. TPAMI, 38(10):2054–2068, 2015
</p>
<p id="5">
[5] Hasith Karunasekera, Han Wang, and Handuo Zhang. Multiple object tracking with attention to appearance, structure, motion and size. IEEE Access 7:104423–104434, 2019.
</p>
<p id="6"> 
[6] Peng Chu and Haibin Ling. Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking. In ICCV, 2019.
</p>
<p id ="7">
[7] Wenwei Zhang, Hui Zhou, Shuyang Sun, Zhe Wang, Jianping Shi, and Chen Change Loy. Robust multi-modality multi-object tracking. In ICCV, 2019.
</p>
<p id="8">
[8] Michael D Breitenstein, Fabian Reichlin, Bastian Leibe, Esther Koller-Meier, and Luc Van Gool. Online multiperson tracking-by-detection from a single, uncalibrated camera. TPAMI, 33(9):1820–1833, 2010.
</p>
<p id="9">
[9] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. Social lstm: Human trajectory prediction in crowded spaces. In CVPR, 2016.
</p>
<p id="10">
[10] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajectories with generative adversarial networks. In CVPR, 2018.
</p>
<p id="11"> 
[11] Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and Sergey Levine. Precog: Prediction conditioned on goals invisual multi-agent settings. In ICCV, 2019. 
</p>
<p id="12">
[12] Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net. In CVPR, 2018.
</p>
<p id="13">
[13] Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun. End-to-end interpretable neural motion planner. In CVPR, 2019.
</p>
<p id="14">
[14]  Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr. Fully-convolutional siamese networks for object tracking. In ECCV, 2016.
</p>
<p id="15">
[15] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In CVPR, 2020.
</p>
<p id="16">
[16] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Realtime 3d object detection from point clouds. In CVPR, 2018.
</p>
<p id = "17">
[17] Keni Bernardin, Alexander Elbs, and Rainer Stiefelhagen. Multiple object tracking performance metrics and evaluation in a smart room environment. In Sixth IEEE International Workshop on Visual Surveillance, in conjunction with ECCV, 2006. 
<p>
<p id="18">
[18] Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu. Class-balanced grouping and sampling for point cloud 3d object detection. arXiv preprint arXiv:1908.09492, 2019
</p>
<p id="19">
[19] Hsu-kuang Chiu, Antonio Prioletti, Jie Li, and Jeannette Bohg. Probabilistic 3d multi-object tracking for autonomous driving. arXiv preprint arXiv:2001.05673, 2020.
</p>
<p id="20">
[20] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, In EMNLP 2014.
</p>
<p id="21">
[21] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. In NIPS 2014 Deep Learning and Representation Learning Workshop.
</p>
<p id="22">
[22] Niamul Quader, Md Mafijul Islam Bhuiyan, Juwei Lu, Peng Dai,and Wei Li. Weight Excitation: Built-in Attention Mechanisms in Convolutional Neural Networks. In ECCV 2020.
</p>
    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/deep-learning/">Deep Learning</a>
  
  <a class="badge badge-light" href="/tags/computer-vision/">Computer Vision</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://abyaadrafid.github.io/post/pnpnet/&amp;text=PnPNet:%20End-to-End%20Perception%20and%20Prediction%20With%20Tracking%20in%20the%20Loop" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://abyaadrafid.github.io/post/pnpnet/&amp;t=PnPNet:%20End-to-End%20Perception%20and%20Prediction%20With%20Tracking%20in%20the%20Loop" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=PnPNet:%20End-to-End%20Perception%20and%20Prediction%20With%20Tracking%20in%20the%20Loop&amp;body=https://abyaadrafid.github.io/post/pnpnet/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://abyaadrafid.github.io/post/pnpnet/&amp;title=PnPNet:%20End-to-End%20Perception%20and%20Prediction%20With%20Tracking%20in%20the%20Loop" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=PnPNet:%20End-to-End%20Perception%20and%20Prediction%20With%20Tracking%20in%20the%20Loop%20https://abyaadrafid.github.io/post/pnpnet/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://abyaadrafid.github.io/post/pnpnet/&amp;title=PnPNet:%20End-to-End%20Perception%20and%20Prediction%20With%20Tracking%20in%20the%20Loop" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      <img class="portrait mr-3" src="https://s.gravatar.com/avatar/da1ba2b74f1da5918090425d903be4e2?s=200')" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://abyaadrafid.github.io/">Rafid Abyaad</a></h5>
      <h6 class="card-subtitle">Masters Student</h6>
      <p class="card-text">I am a masters student at TU Munich focusing on Machine learning, Robotics and Formal Methods.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:rafid.abyaad@tum.de" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://kaggle.com/abyaadrafid" target="_blank" rel="noopener">
        <i class="fab fa-kaggle"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://linkedin.com/in/abyaadrafid" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/abyaadrafid" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/project/aptos/">APTOS Blindness Detection</a></li>
      
      <li><a href="/project/acic/">Aerial Cactus Identification</a></li>
      
      <li><a href="/project/f360/">Computer Vision 101</a></li>
      
      <li><a href="/project/ferm/">Facial Expression Recognition</a></li>
      
      <li><a href="/project/rcic/">Recursion Cellular Image Classification</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.3.1/mermaid.min.js" integrity="sha256-vOIuDSYDirTfyr+S2MjFnhOz6Rgiz4ODFAHATG0rFxw=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3227ab49eed49815d1b4ba40154f74e7.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
