<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.6.3">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Rafid Abyaad">

  
  
  
    
  
  <meta name="description" content="In this post, we will be looking at the paper PnPNet: End-to-End Perception and Prediction with Tracking in the Loop, by Liang et al., which was published in CVPR 2020 [1]. After defining our task and discussing some related research in this field, we will be looking at the methodology of the paper. Then we will analyse the quantitative results and have a look at the qualitative results. Finally, we will finish it off with some remarks and possible ideas for extension.">

  
  <link rel="alternate" hreflang="en-us" href="https://abyaadrafid.github.io/post/pnpnet/">

  


  
  
  
  <meta name="theme-color" content="rgb(0, 136, 204)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://abyaadrafid.github.io/post/pnpnet/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Homepage | Rafid">
  <meta property="og:url" content="https://abyaadrafid.github.io/post/pnpnet/">
  <meta property="og:title" content="PnPNet: End-to-End Perception and Prediction With Tracking in the Loop | Homepage | Rafid">
  <meta property="og:description" content="In this post, we will be looking at the paper PnPNet: End-to-End Perception and Prediction with Tracking in the Loop, by Liang et al., which was published in CVPR 2020 [1]. After defining our task and discussing some related research in this field, we will be looking at the methodology of the paper. Then we will analyse the quantitative results and have a look at the qualitative results. Finally, we will finish it off with some remarks and possible ideas for extension."><meta property="og:image" content="https://abyaadrafid.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://abyaadrafid.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2021-04-25T19:36:52&#43;06:00">
    
    <meta property="article:modified_time" content="2021-04-25T19:36:52&#43;06:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://abyaadrafid.github.io/post/pnpnet/"
  },
  "headline": "PnPNet: End-to-End Perception and Prediction With Tracking in the Loop",
  
  "datePublished": "2021-04-25T19:36:52+06:00",
  "dateModified": "2021-04-25T19:36:52+06:00",
  
  "author": {
    "@type": "Person",
    "name": "Rafid Abyaad"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Homepage | Rafid",
    "logo": {
      "@type": "ImageObject",
      "url": "https://abyaadrafid.github.io/img/icon-512.png"
    }
  },
  "description": "In this post, we will be looking at the paper PnPNet: End-to-End Perception and Prediction with Tracking in the Loop, by Liang et al., which was published in CVPR 2020 [1]. After defining our task and discussing some related research in this field, we will be looking at the methodology of the paper. Then we will analyse the quantitative results and have a look at the qualitative results. Finally, we will finish it off with some remarks and possible ideas for extension."
}
</script>

  

  


  


  





  <title>PnPNet: End-to-End Perception and Prediction With Tracking in the Loop | Homepage | Rafid</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  

<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Homepage | Rafid</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Homepage | Rafid</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#skills"><span>Skills</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#experience"><span>Experience</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/Abyaad_CV.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>PnPNet: End-to-End Perception and Prediction With Tracking in the Loop</h1>

  
  <p class="page-subtitle">A review of a CVPR paper in the paradigm of autonomous driving</p>
  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 25, 2021
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    5 min read
  </span>
  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>In this post, we will be looking at the paper <a href="https://arxiv.org/abs/2005.14711">PnPNet: End-to-End Perception and Prediction with Tracking in the Loop</a>, by <em>Liang et al.</em>,  which was published in CVPR 2020 <a href="http://abyaadrafid.github.io/post/pnpnet/#1">[1]</a>. After defining our task and discussing some related research in this field, we will be looking at the methodology of the paper. Then we will analyse the quantitative results and have a look at the qualitative results. Finally, we will finish it off with some remarks and possible ideas for extension.</p>
<h1 id="introduction">Introduction</h1>
<p>In the context of self-driving vehicles, predicting the motion of other vehicles is a critical task. Approximating the trajectory of neighbouring agents in the future is equally as important as detecting them in the current time frame. To do this task, so far three paradigms have been proposed.</p>
<p>The first divides this problem into three separate sub-tasks, which are handled by completely independent sub-systems. These three tasks, namely, object detection, object tracking, and motion forecasting are done sequentially. As they are developed separately, they need more computing power and cannot correct mistakes from upstream tasks. The second paradigm  tries to solve the detection and prediction task with a single neural network. This yields more efficient computation but these models suffer from limited use of temporal history and are vulnerable to occlusion.</p>
<p><img src="autonomy_stacks.png" alt="image"></p>
<figcaption>
<p>Three paradigms of perception and prediction problems <a href="http://abyaadrafid.github.io/post/pnpnet/#1">[1]</a></p>
</figcaption>
<p>This paper introduces the new third paradigm. It argues that, for sequential modelling tasks such as motion forecasting, past data is very important. To that end, it proposes PnPNet which combines multi-object tracking with joint perception and prediction models. We will go into the details of the model after discussing some other works that try to tackle different aspects of our problem.</p>
<h1 id="related-works">Related works</h1>
<h3 id="3d-object-detection">3D object detection</h3>
<p>The use of depth sensors such as LiDARs have been shown to have better performance <a href="http://abyaadrafid.github.io/post/pnpnet/#2">[2]</a> than cameras for 3D detection. Some works also explore a fusion of LiDAR point clouds and camera inputs <a href="http://abyaadrafid.github.io/post/pnpnet/#3">[3]</a>.
<img src="fusion_model.png" alt="image"></p>
<figcaption>
<p>A qualitative result of fusion models <a href="http://abyaadrafid.github.io/post/pnpnet/#3">[3]</a></p>
</figcaption>
<h3 id="multi-object-tracking">Multi-Object Tracking</h3>
<p>Multi-Object tracking is a system to track multiple objects at the same time. It consists of a discrete data association problem and a continuous trajectory estimation problem <a href="http://abyaadrafid.github.io/post/pnpnet/#4">[4]</a>. There have been efforts to handle occlusion with hand crafted heuristics <a href="http://abyaadrafid.github.io/post/pnpnet/#5">[5]</a> and single object tracking <a href="http://abyaadrafid.github.io/post/pnpnet/#6">[6]</a> to handle occlusion. To handle the trajectory problem, some approaches also use sensor features but only use up to 3 seconds of temporal history <a href="http://abyaadrafid.github.io/post/pnpnet/#7">[7]</a>.</p>
<p><img src="multi-object_tracking.png" alt="image"></p>
<figcaption>
<p>An example of multi-object tracking takes from <a href="http://abyaadrafid.github.io/post/pnpnet/#8">[8]</a></p>
</figcaption>
<h3 id="motion-forecasting">Motion Forecasting</h3>
<p>Different methods try to approach the multi agent motion forecasting problem. <em>Alahi et al.</em> propose lstm based social pooling to model motion <a href="http://abyaadrafid.github.io/post/pnpnet/#9">[9]</a>. Social-GAN <a href="http://abyaadrafid.github.io/post/pnpnet/#10">[10]</a> improves on it using adversarial training. The use of sensor features are also explored, but these methods usually have generalization issues when applied to noisy data <a href="http://abyaadrafid.github.io/post/pnpnet/#11">[11]</a>.</p>
<h3 id="joint-models-for-perception-and-prediction">Joint models for Perception and Prediction</h3>
<p>The FAF paper <a href="http://abyaadrafid.github.io/post/pnpnet/#12">[12]</a> of <em>Luo et al.</em> serves as a direct predecessor and an evaluation baseline for PnPNet. This model uses a single convolutional backbone to detect and predict future motion. NeuralMP <a href="http://abyaadrafid.github.io/post/pnpnet/#13">[13]</a> shares motion planning features with perception and prediction to allow end to end training.</p>
<p><img src="neuralmp.png" alt="image"></p>
<figcaption>
<p>Some qualitative results from NeuralMP <a href="http://abyaadrafid.github.io/post/pnpnet/#13">[13]</a></p>
</figcaption>
<h1 id="proposed-method--pnpnet">Proposed method : PnPNet</h1>
<h3 id="technical-contributions">Technical Contributions</h3>
<p>The discussed related research overlook an important aspect of our problem. The do not take temporal characteristics of actors into account. To allow for that, this paper makes two major technical contributions :</p>
<ol>
<li>It introduces a new trajectory representation based on a sequence of detections through time.</li>
<li>It proposes a multi-object tracker that solves both the association and the trajectory estimation problem.</li>
</ol>
<h2 id="model-overview">Model Overview</h2>
<p>PnPNet consists of three separate modules :</p>
<ol>
<li>3D Object detection module</li>
<li>Discrete-Continuous Tracking module</li>
<li>Motion forecasting module</li>
</ol>
<p><img src="model.png" alt="image"></p>
<figcaption>
<p>A summary of PnPNet workflow <a href="http://abyaadrafid.github.io/post/pnpnet/#1">[1]</a></p>
</figcaption>
<h2 id="3d-object-detection-module">3D Object detection module</h2>
<p>The detection module takes multi-sweep LiDAR point cloud representation in bird-eye-view and an HD map as input. Optionally, geometric and semantic information of the HD map can also be used. 2D convolutional neural network based backbone is applied to the input, which generates the intermediate BEV features that will be used in downstream tasks. A convolutional detection header is then used on the intermediate features to create dense object detections at each time step.</p>
<p><img src="3dmodule.png" alt="image"></p>
<figcaption>
<p>Workflow of the 3D detection module</p>
</figcaption>
<h2 id="discrete-continuous-tracking-module">Discrete-Continuous Tracking module</h2>
<h2 id="motion-forecasting-module">Motion Forecasting module</h2>
<h1 id="references">References</h1>
<div id="1">
[1] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, Raquel Urtasun, "PnPNet: End-to-End Perception and Prediction with Tracking in the Loop", in CVPR, 2020.
</div>
<div id="2">
[2] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Realtime 3d object detection from point clouds. In CVPR, 2018.
</div>
<div id="3">
[3] Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun. Deep continuous fusion for multi-sensor 3d object detection. In ECCV, 2018.
</div>
<div id="4">
[4] Anton Milan, Konrad Schindler, and Stefan Roth. Multitarget tracking by discrete-continuous energy minimization. TPAMI, 38(10):2054–2068, 2015
</div>
<div id="5">
[5] Hasith Karunasekera, Han Wang, and Handuo Zhang. Multiple object tracking with attention to appearance, structure, motion and size. IEEE Access 7:104423–104434, 2019.
</div>
<div id="6"> 
[6] Peng Chu and Haibin Ling. Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking. In ICCV, 2019.
</div>
<div id ="7">
[7] Wenwei Zhang, Hui Zhou, Shuyang Sun, Zhe Wang, Jianping Shi, and Chen Change Loy. Robust multi-modality multi-object tracking. In ICCV, 2019.
</div>
<div id="8">
[8] Michael D Breitenstein, Fabian Reichlin, Bastian Leibe, Esther Koller-Meier, and Luc Van Gool. Online multiperson tracking-by-detection from a single, uncalibrated camera. TPAMI, 33(9):1820–1833, 2010.
</div>
<div id="9">
[9] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. Social lstm: Human trajectory prediction in crowded spaces. In CVPR, 2016.
</div>
<div id="10">
[10] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajectories with generative adversarial networks. In CVPR, 2018.
</div>
<div id="11">
[11] Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and Sergey Levine. Precog: Prediction conditioned on goals invisual multi-agent settings. In ICCV, 2019. 
</div>
<div id="12">
[12] Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net. In CVPR, 2018.
</div>
<div id=13>
[13] Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun. End-to-end interpretable neural motion planner. In CVPR, 2019.
</div>
    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/deep-learning/">Deep Learning</a>
  
  <a class="badge badge-light" href="/tags/computer-vision/">Computer Vision</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://abyaadrafid.github.io/post/pnpnet/&amp;text=PnPNet:%20End-to-End%20Perception%20and%20Prediction%20With%20Tracking%20in%20the%20Loop" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://abyaadrafid.github.io/post/pnpnet/&amp;t=PnPNet:%20End-to-End%20Perception%20and%20Prediction%20With%20Tracking%20in%20the%20Loop" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=PnPNet:%20End-to-End%20Perception%20and%20Prediction%20With%20Tracking%20in%20the%20Loop&amp;body=https://abyaadrafid.github.io/post/pnpnet/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://abyaadrafid.github.io/post/pnpnet/&amp;title=PnPNet:%20End-to-End%20Perception%20and%20Prediction%20With%20Tracking%20in%20the%20Loop" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=PnPNet:%20End-to-End%20Perception%20and%20Prediction%20With%20Tracking%20in%20the%20Loop%20https://abyaadrafid.github.io/post/pnpnet/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://abyaadrafid.github.io/post/pnpnet/&amp;title=PnPNet:%20End-to-End%20Perception%20and%20Prediction%20With%20Tracking%20in%20the%20Loop" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      <img class="portrait mr-3" src="https://s.gravatar.com/avatar/da1ba2b74f1da5918090425d903be4e2?s=200')" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://abyaadrafid.github.io/">Rafid Abyaad</a></h5>
      <h6 class="card-subtitle">Masters Student</h6>
      <p class="card-text">I am a masters student at TU Munich focusing on Machine learning, Robotics and Formal Methods.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:rafid.abyaad@tum.de" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://kaggle.com/abyaadrafid" target="_blank" rel="noopener">
        <i class="fab fa-kaggle"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://linkedin.com/in/abyaadrafid" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/abyaadrafid" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/project/aptos/">APTOS Blindness Detection</a></li>
      
      <li><a href="/project/acic/">Aerial Cactus Identification</a></li>
      
      <li><a href="/project/f360/">Computer Vision 101</a></li>
      
      <li><a href="/project/ferm/">Facial Expression Recognition</a></li>
      
      <li><a href="/project/rcic/">Recursion Cellular Image Classification</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

      
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3227ab49eed49815d1b4ba40154f74e7.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
