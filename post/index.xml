<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Homepage | Rafid</title>
    <link>https://abyaadrafid.github.io/post/</link>
      <atom:link href="https://abyaadrafid.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 25 Apr 2021 19:36:52 +0600</lastBuildDate>
    <image>
      <url>https://abyaadrafid.github.io/img/icon-192.png</url>
      <title>Posts</title>
      <link>https://abyaadrafid.github.io/post/</link>
    </image>
    
    <item>
      <title>PnPNet: End-to-End Perception and Prediction With Tracking in the Loop</title>
      <link>https://abyaadrafid.github.io/post/pnpnet/</link>
      <pubDate>Sun, 25 Apr 2021 19:36:52 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/post/pnpnet/</guid>
      <description>&lt;p&gt;In this post, we will be looking at the paper &lt;a href=&#34;https://arxiv.org/abs/2005.14711&#34;&gt;PnPNet: End-to-End Perception and Prediction with Tracking in the Loop&lt;/a&gt;, by &lt;em&gt;Liang et al.&lt;/em&gt;,  which was published in CVPR 2020 &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#1&#34;&gt;[1]&lt;/a&gt;. After defining our task and discussing some related research in this field, we will be looking at the methodology of the paper. Then we will analyze the quantative results and have a look at the qualitative results. Finally, we will finish it off with some remarks and possible ideas for extension.&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In the context of self-driving vehicles, predicting the motion of other vehicles is a critical task. Approximating the trajectory of neighboring agents in the future is equally as important as detecting them in the current time frame. To do this task, so far three paradigms have been proposed.&lt;/p&gt;
&lt;p&gt;The first divides this problem into three separate sub-tasks, which are handled by completely independent sub-systems. These three tasks, namely, object detection, object tracking, and motion forecasting are done sequentially. As they are developed separately, they need more computing power and cannot correct mistakes from upstream tasks. The second paradigm  tries to solve the detection and prediction task with a single neural network. This yields more efficient computation but these models suffer from limited use of temporal history and are vulnerable to occlusion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;autonomy_stacks.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;p&gt;Three paradigms of perception and prediction problems &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#1&#34;&gt;[1]&lt;/a&gt;&lt;/p&gt;
&lt;/figcaption&gt;
&lt;p&gt;This paper introduces the new third paradigm. It argues that, for sequential modeling tasks such as motion forecasting, past data is very important. To that end, it proposes PnPNet which combines multi-object tracking with joint perception and prediction models. We will go into the details of the model after discussing some other works that try to tackle different aspects of our problem.&lt;/p&gt;
&lt;h1 id=&#34;related-works&#34;&gt;Related works&lt;/h1&gt;
&lt;h3 id=&#34;3d-object-detection&#34;&gt;3D object detection&lt;/h3&gt;
&lt;p&gt;The use of depth sensors such as LiDARs have been shown to have better performance &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#2&#34;&gt;[2]&lt;/a&gt; than cameras for 3D detection. Some works also explore a fusion of LiDAR point clouds and camera inputs &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#3&#34;&gt;[3]&lt;/a&gt;.
&lt;img src=&#34;fusion_model.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;p&gt;A qualitative result of fusion models &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#3&#34;&gt;[3]&lt;/a&gt;&lt;/p&gt;
&lt;/figcaption&gt;
&lt;h3 id=&#34;multi-object-tracking&#34;&gt;Multi-Object Tracking&lt;/h3&gt;
&lt;p&gt;Multi-Object tracking is a system to track multiple objects at the same time. It consists of a discrete data association problem and a continuous trajectory estimation problem &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#4&#34;&gt;[4]&lt;/a&gt;. There have been efforts to handle occulsions with hand crafted heuristics &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#5&#34;&gt;[5]&lt;/a&gt; and single object tracking &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#6&#34;&gt;[6]&lt;/a&gt; to handle occulsion. To handle the trajectory problem, some approaches also use sensor features but only use upto 3 seconds of temporal history &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#7&#34;&gt;[7]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;multi-object_tracking.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;p&gt;An example of multi-object tracking takes from &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#8&#34;&gt;[8]&lt;/a&gt;&lt;/p&gt;
&lt;/figcaption&gt;
&lt;h3 id=&#34;motion-forecasting&#34;&gt;Motion Forecasting&lt;/h3&gt;
&lt;h3 id=&#34;joint-models-for-perception-and-prediction&#34;&gt;Joint models for Perception and Prediction&lt;/h3&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;div id=&#34;1&#34;&gt;
[1] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, Raquel Urtasun, &#34;PnPNet: End-to-End Perception and Prediction with Tracking in the Loop&#34;, in CVPR, 2020.
&lt;/div&gt;
&lt;div id=&#34;2&#34;&gt;
[2] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Realtime 3d object detection from point clouds. In CVPR, 2018.
&lt;/div&gt;
&lt;div id=&#34;3&#34;&gt;
[3] Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun. Deep continuous fusion for multi-sensor 3d object detection. In ECCV, 2018.
&lt;/div&gt;
&lt;div id=&#34;4&#34;&gt;
[4] Anton Milan, Konrad Schindler, and Stefan Roth. Multitarget tracking by discrete-continuous energy minimization. TPAMI, 38(10):2054–2068, 2015
&lt;/div&gt;
&lt;div id=&#34;5&#34;&gt;
Hasith Karunasekera, Han Wang, and Handuo Zhang. Multiple object tracking with attention to appearance, structure, motion and size. IEEE Access 7:104423–104434, 2019.
&lt;/div&gt;
&lt;div id=&#34;6&#34;&gt; 
Peng Chu and Haibin Ling. Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking. In ICCV, 2019.
&lt;/div&gt;
&lt;div id =&#34;7&#34;&gt;
Wenwei Zhang, Hui Zhou, Shuyang Sun, Zhe Wang, Jianping Shi, and Chen Change Loy. Robust multi-modality multi-object tracking. In ICCV, 2019.
&lt;/div&gt;
&lt;div id=&#34;8&#34;&gt;
Michael D Breitenstein, Fabian Reichlin, Bastian Leibe, Esther Koller-Meier, and Luc Van Gool. Online multiperson tracking-by-detection from a single, uncalibrated camera. TPAMI, 33(9):1820–1833, 2010.
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
