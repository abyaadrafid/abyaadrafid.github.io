<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision | Homepage | Rafid</title>
    <link>https://abyaadrafid.github.io/tags/computer-vision/</link>
      <atom:link href="https://abyaadrafid.github.io/tags/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    <description>Computer Vision</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 25 Apr 2021 19:36:52 +0600</lastBuildDate>
    <image>
      <url>https://abyaadrafid.github.io/img/icon-192.png</url>
      <title>Computer Vision</title>
      <link>https://abyaadrafid.github.io/tags/computer-vision/</link>
    </image>
    
    <item>
      <title>PnPNet: End-to-End Perception and Prediction With Tracking in the Loop</title>
      <link>https://abyaadrafid.github.io/post/pnpnet/</link>
      <pubDate>Sun, 25 Apr 2021 19:36:52 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/post/pnpnet/</guid>
      <description>&lt;p&gt;In this post, we will be looking at the paper &lt;a href=&#34;https://arxiv.org/abs/2005.14711&#34;&gt;PnPNet: End-to-End Perception and Prediction with Tracking in the Loop&lt;/a&gt;, by &lt;em&gt;Liang et al.&lt;/em&gt;,  which was published in CVPR 2020 &lt;a href=&#34;http://localhost:1313/post/pnpnet/#1&#34;&gt;[1]&lt;/a&gt;. After discuss our task and discussing some related research in this field, we will be looking at the methodology of the paper. Then we will analyze the quantative results and have a look at the qualitative results. Finally, we will finish it off with some remarks and possible ideas for extension.&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque gravida dignissim suscipit. Integer quis faucibus felis. Pellentesque consectetur tellus odio, ornare ultrices massa ultrices nec. Nam sit amet tincidunt eros. Phasellus id posuere est. Sed accumsan accumsan risus vel posuere. Morbi at nibh ultricies, ultrices nisi non, congue nisl. Phasellus rhoncus ligula vel sem iaculis, sed bibendum lectus commodo. Fusce in aliquam tortor. Ut iaculis sapien enim, non mollis lectus cursus vitae. Aenean dictum ex at arcu volutpat elementum. Ut in elit molestie, convallis mauris id, mollis felis. Ut mattis leo non elit fermentum, vel interdum massa faucibus. Maecenas elit nibh, luctus ut massa sit amet, efficitur ornare magna.&lt;/p&gt;
&lt;p&gt;Curabitur viverra, mi eu vestibulum cursus, tellus nibh lobortis ex, at congue felis ante in ante. Aliquam erat volutpat. Nulla facilisi. Sed id gravida est. Aliquam sem enim, rutrum ac rutrum id, pharetra quis sapien. Quisque malesuada lacus eget tortor pharetra ullamcorper. Integer non metus eros. Curabitur ornare efficitur sollicitudin. Nullam pharetra nisl sed leo scelerisque, ut cursus turpis fermentum.&lt;/p&gt;
&lt;p&gt;Aliquam id convallis lorem, vehicula fringilla nisl. Sed euismod porttitor tempor. Vivamus at tortor quis ex faucibus ullamcorper sit amet a orci. Nullam nisi metus, rutrum at urna quis, lobortis venenatis orci. Nunc mollis lacus tortor, vitae pharetra dolor suscipit vitae. Praesent nec malesuada purus. Duis id magna eu lacus elementum faucibus.&lt;/p&gt;
&lt;p&gt;Quisque scelerisque lectus dui. Fusce id ipsum sit amet ante consectetur consequat et ut justo. Mauris orci elit, sagittis in posuere vitae, euismod dignissim diam. Nulla ac velit pellentesque, gravida arcu tristique, feugiat elit. Quisque ultricies ut est non imperdiet. Vivamus blandit luctus ligula sagittis consequat. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Ut eu velit ac sapien convallis luctus.&lt;/p&gt;
&lt;p&gt;Proin ornare non enim at dapibus. Aliquam erat volutpat. Aenean malesuada pharetra lacus, euismod suscipit mauris gravida quis. Morbi id pulvinar libero. Pellentesque imperdiet dignissim gravida. Morbi commodo augue non dolor lacinia aliquam. Nulla erat lorem, aliquam nec lacinia eu, efficitur in lacus.&lt;/p&gt;
&lt;h1 id=&#34;related-works&#34;&gt;Related works&lt;/h1&gt;
&lt;div id=&#34;1&#34;&gt;
[1] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, Raquel Urtasun, &#34;PnPNet: End-to-End Perception and Prediction with Tracking in the Loop&#34;, in CVPR, 2020.
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Aerial Cactus Identification</title>
      <link>https://abyaadrafid.github.io/project/acic/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/acic/</guid>
      <description>&lt;p&gt;This was one my of very first competitions. While doing &lt;a href=&#34;https://course.fast.ai/&#34;&gt;Practical Deep Learning for Coders&lt;/a&gt;, this competition provided a good source of practise. It was a binary classification problem. The goal for this competition was to determine whether the given satellite image contained a columnur cactus.&lt;/p&gt;
&lt;p&gt;I used this dataset for two purposes :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To implement and test &lt;a href=&#34;https://arxiv.org/pdf/1801.07698.pdf&#34;&gt;ArcFace&lt;/a&gt; using pytorch.&lt;/li&gt;
&lt;li&gt;To get placed into a high LeaderBoard position in the competition using FastAI.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;h3 id=&#34;eda&#34;&gt;EDA&lt;/h3&gt;
&lt;p&gt;According to the dataset details, the images were taken from the air. The images are low-res, some of them rotated to arbitrary angles and some zoomed. From visual inspection, the cacti are somewhat easy to spot because of their unique texture and stick-like shape. The class imbalance is not severe, can be handled by data augmentation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;data_print.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;data-split-and-transforms&#34;&gt;Data split and Transforms&lt;/h3&gt;
&lt;h4 id=&#34;split&#34;&gt;Split&lt;/h4&gt;
&lt;p&gt;As the class imbalance was not servere, the data could be split into train/valid set at random.&lt;/p&gt;
&lt;h4 id=&#34;transforms&#34;&gt;Transforms&lt;/h4&gt;
&lt;p&gt;Following Transforms were applied with 75% probability to augment the data, then the images were resized to 128*128. Test time augmentation was not applied.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Horizontal Flip&lt;/li&gt;
&lt;li&gt;Vertical Flip&lt;/li&gt;
&lt;li&gt;Left and Right rotation upto 10Â°&lt;/li&gt;
&lt;li&gt;Upto 110% zoom&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;hyperparameters&#34;&gt;Hyperparameters&lt;/h3&gt;
&lt;h4 id=&#34;arcface&#34;&gt;ArcFace&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;s = 64&lt;/li&gt;
&lt;li&gt;m = 0.0&lt;/li&gt;
&lt;li&gt;Adam Optimizer with fixed lr = 1e-3&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;competition-classifiers&#34;&gt;Competition Classifiers&lt;/h4&gt;
&lt;h5 id=&#34;densenet169&#34;&gt;Densenet169&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;Frozen model, Adam optimizer with maximum lr = 7.5e-3.&lt;/li&gt;
&lt;li&gt;CyclirLR scheduler&lt;/li&gt;
&lt;li&gt;Unfrozen model, Adam optimizer with maximum lr = 1e-6.&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;resnet101&#34;&gt;Resnet101&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;Frozen model, Adam optimizer with maximum lr = 9e-3.&lt;/li&gt;
&lt;li&gt;CyclirLR scheduler&lt;/li&gt;
&lt;li&gt;Unfrozen model, Adam optimizer with maximum lr = 1e-6.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;model-performance&#34;&gt;Model Performance&lt;/h3&gt;
&lt;p&gt;I used DenseNet169 and Resnet101 for Leaderboard and ArcFace for research purposes.&lt;/p&gt;
&lt;h3 id=&#34;arcface-1&#34;&gt;ArcFace&lt;/h3&gt;
&lt;p&gt;ArcFace was applied on the Resnet101 backbone. Implemented from scratch in pytorch. With embedding dimension = 2048 and scale_factor (s) = 64, accuracy follows :&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;arcface.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;Further experimentation using additional linear layers can boost the results. Then again, this approach is designed for one-shot learning. Worse performance in Binary Classification is quite understandable.&lt;/p&gt;
&lt;h4 id=&#34;densenet169-1&#34;&gt;DenseNet169&lt;/h4&gt;
&lt;p&gt;Densenet169 needs more time to converge because of its enormous size and paramters.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;epoch&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;train_loss&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;valid_loss&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;error_rate&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;accuracy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.059754&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.004154&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:35&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.062731&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000837&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.019187&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.003954&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.009922&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000457&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.004491&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000055&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:27&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;resnet101-1&#34;&gt;Resnet101&lt;/h4&gt;
&lt;p&gt;Resnet101 needed less time to converge.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;epoch&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;train_loss&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;valid_loss&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;error_rate&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;accuracy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.063169&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.033260&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.011429&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.988571&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.034835&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.002770&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.024171&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.002123&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.014281&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.006416&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.005714&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.994286&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.006923&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.002465&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:13&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;competition-standings&#34;&gt;Competition Standings&lt;/h2&gt;
&lt;p&gt;My models acheived perfect accuracy score in the public leaderboard.&lt;/p&gt;
&lt;form action=&#34;https://github.com/abyaadrafid/Aerial-Cactus-Identification&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>APTOS Blindness Detection</title>
      <link>https://abyaadrafid.github.io/project/aptos/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/aptos/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/APTOS-Blindness-Detection&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Computer Vision 101</title>
      <link>https://abyaadrafid.github.io/project/f360/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/f360/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Fruits-360&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Facial Expression Recognition</title>
      <link>https://abyaadrafid.github.io/project/ferm/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/ferm/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Face-Expression-Recognition&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Recursion Cellular Image Classification</title>
      <link>https://abyaadrafid.github.io/project/rcic/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/rcic/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Recursion-Cellular-Image-Classification&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>SIIM-ACR Pneumothorax Segmentation</title>
      <link>https://abyaadrafid.github.io/project/siim-acr/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/siim-acr/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/SIIM-ACR-Pneumothorax-Segmentation&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Tabular vs Vision Models</title>
      <link>https://abyaadrafid.github.io/project/dmnist/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/dmnist/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Digits-MNIST&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
  </channel>
</rss>
