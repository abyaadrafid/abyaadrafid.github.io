<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Homepage | Rafid</title>
    <link>https://abyaadrafid.github.io/</link>
      <atom:link href="https://abyaadrafid.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Homepage | Rafid</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 25 Apr 2021 19:36:52 +0600</lastBuildDate>
    <image>
      <url>https://abyaadrafid.github.io/img/icon-192.png</url>
      <title>Homepage | Rafid</title>
      <link>https://abyaadrafid.github.io/</link>
    </image>
    
    <item>
      <title>PnPNet: End-to-End Perception and Prediction With Tracking in the Loop</title>
      <link>https://abyaadrafid.github.io/post/pnpnet/</link>
      <pubDate>Sun, 25 Apr 2021 19:36:52 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/post/pnpnet/</guid>
      <description>&lt;p&gt;In this post, we will be looking at the paper &lt;a href=&#34;https://arxiv.org/abs/2005.14711&#34;&gt;PnPNet: End-to-End Perception and Prediction with Tracking in the Loop&lt;/a&gt;, by &lt;em&gt;Liang et al.&lt;/em&gt;,  which was published in CVPR 2020 &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#1&#34;&gt;[1]&lt;/a&gt;. After defining our task and discussing some related research in this field, we will be looking at the methodology of the paper. Then we will analyse the quantitative results and have a look at the qualitative results. Finally, we will finish it off with some remarks and possible ideas for extension.&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In the context of self-driving vehicles, predicting the motion of other vehicles is a critical task. Approximating the trajectory of neighbouring agents in the future is equally as important as detecting them in the current time frame. To do this task, so far three paradigms have been proposed.&lt;/p&gt;
&lt;p&gt;The first divides this problem into three separate sub-tasks, which are handled by completely independent sub-systems. These three tasks, namely, object detection, object tracking, and motion forecasting are done sequentially. As they are developed separately, they need more computing power and cannot correct mistakes from upstream tasks. The second paradigm  tries to solve the detection and prediction task with a single neural network. This yields more efficient computation but these models suffer from limited use of temporal history and are vulnerable to occlusion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;autonomy_stacks.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;p&gt;Three paradigms of perception and prediction problems &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#1&#34;&gt;[1]&lt;/a&gt;&lt;/p&gt;
&lt;/figcaption&gt;
&lt;p&gt;This paper introduces the new third paradigm. It argues that, for sequential modelling tasks such as motion forecasting, past data is very important. To that end, it proposes PnPNet which combines multi-object tracking with joint perception and prediction models. We will go into the details of the model after discussing some other works that try to tackle different aspects of our problem.&lt;/p&gt;
&lt;h1 id=&#34;related-works&#34;&gt;Related works&lt;/h1&gt;
&lt;h3 id=&#34;3d-object-detection&#34;&gt;3D object detection&lt;/h3&gt;
&lt;p&gt;The use of depth sensors such as LiDARs have been shown to have better performance &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#2&#34;&gt;[2]&lt;/a&gt; than cameras for 3D detection. Some works also explore a fusion of LiDAR point clouds and camera inputs &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#3&#34;&gt;[3]&lt;/a&gt;.
&lt;img src=&#34;fusion_model.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;p&gt;A qualitative result of fusion models &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#3&#34;&gt;[3]&lt;/a&gt;&lt;/p&gt;
&lt;/figcaption&gt;
&lt;h3 id=&#34;multi-object-tracking&#34;&gt;Multi-Object Tracking&lt;/h3&gt;
&lt;p&gt;Multi-Object tracking is a system to track multiple objects at the same time. It consists of a discrete data association problem and a continuous trajectory estimation problem &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#4&#34;&gt;[4]&lt;/a&gt;. There have been efforts to handle occlusion with hand crafted heuristics &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#5&#34;&gt;[5]&lt;/a&gt; and single object tracking &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#6&#34;&gt;[6]&lt;/a&gt; to handle occlusion. To handle the trajectory problem, some approaches also use sensor features but only use up to 3 seconds of temporal history &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#7&#34;&gt;[7]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;multi-object_tracking.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;p&gt;An example of multi-object tracking takes from &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#8&#34;&gt;[8]&lt;/a&gt;&lt;/p&gt;
&lt;/figcaption&gt;
&lt;h3 id=&#34;motion-forecasting&#34;&gt;Motion Forecasting&lt;/h3&gt;
&lt;p&gt;Different methods try to approach the multi agent motion forecasting problem. &lt;em&gt;Alahi et al.&lt;/em&gt; propose lstm based social pooling to model motion &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#9&#34;&gt;[9]&lt;/a&gt;. Social-GAN &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#10&#34;&gt;[10]&lt;/a&gt; improves on it using adversarial training. The use of sensor features are also explored, but these methods usually have generalization issues when applied to noisy data &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#11&#34;&gt;[11]&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;joint-models-for-perception-and-prediction&#34;&gt;Joint models for Perception and Prediction&lt;/h3&gt;
&lt;p&gt;The FAF paper &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#12&#34;&gt;[12]&lt;/a&gt; of &lt;em&gt;Luo et al.&lt;/em&gt; serves as a direct predecessor and an evaluation baseline for PnPNet. This model uses a single convolutional backbone to detect and predict future motion. NeuralMP &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#13&#34;&gt;[13]&lt;/a&gt; shares motion planning features with perception and prediction to allow end to end training.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;neuralmp.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;p&gt;Some qualitative results from NeuralMP &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#13&#34;&gt;[13]&lt;/a&gt;&lt;/p&gt;
&lt;/figcaption&gt;
&lt;h1 id=&#34;proposed-method--pnpnet&#34;&gt;Proposed method : PnPNet&lt;/h1&gt;
&lt;h3 id=&#34;technical-contributions&#34;&gt;Technical Contributions&lt;/h3&gt;
&lt;p&gt;The discussed related research overlook an important aspect of our problem. The do not take temporal characteristics of actors into account. To allow for that, this paper makes two major technical contributions :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It introduces a new trajectory representation based on a sequence of detections through time.&lt;/li&gt;
&lt;li&gt;It proposes a multi-object tracker that solves both the association and the trajectory estimation problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;model-overview&#34;&gt;Model Overview&lt;/h2&gt;
&lt;p&gt;PnPNet consists of three separate modules :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;3D Object detection module&lt;/li&gt;
&lt;li&gt;Discrete-Continuous Tracking module&lt;/li&gt;
&lt;li&gt;Motion forecasting module&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;model.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;p&gt;A summary of PnPNet workflow &lt;a href=&#34;http://abyaadrafid.github.io/post/pnpnet/#1&#34;&gt;[1]&lt;/a&gt;&lt;/p&gt;
&lt;/figcaption&gt;
&lt;h2 id=&#34;3d-object-detection-module&#34;&gt;3D Object detection module&lt;/h2&gt;
&lt;p&gt;The detection module takes multi-sweep LiDAR point cloud representation in bird-eye-view and an HD map as input. Optionally, geometric and semantic information of the HD map can also be used. 2D convolutional neural network based backbone is applied to the input, which generates the intermediate BEV features that will be used in downstream tasks. A convolutional detection header is then used on the intermediate features to create dense object detections at each time step.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;3dmodule.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;p&gt;Workflow of the 3D detection module&lt;/p&gt;
&lt;/figcaption&gt;
&lt;h2 id=&#34;discrete-continuous-tracking-module&#34;&gt;Discrete-Continuous Tracking module&lt;/h2&gt;
&lt;h2 id=&#34;motion-forecasting-module&#34;&gt;Motion Forecasting module&lt;/h2&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;div id=&#34;1&#34;&gt;
[1] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, Raquel Urtasun, &#34;PnPNet: End-to-End Perception and Prediction with Tracking in the Loop&#34;, in CVPR, 2020.
&lt;/div&gt;
&lt;div id=&#34;2&#34;&gt;
[2] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Realtime 3d object detection from point clouds. In CVPR, 2018.
&lt;/div&gt;
&lt;div id=&#34;3&#34;&gt;
[3] Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun. Deep continuous fusion for multi-sensor 3d object detection. In ECCV, 2018.
&lt;/div&gt;
&lt;div id=&#34;4&#34;&gt;
[4] Anton Milan, Konrad Schindler, and Stefan Roth. Multitarget tracking by discrete-continuous energy minimization. TPAMI, 38(10):2054–2068, 2015
&lt;/div&gt;
&lt;div id=&#34;5&#34;&gt;
[5] Hasith Karunasekera, Han Wang, and Handuo Zhang. Multiple object tracking with attention to appearance, structure, motion and size. IEEE Access 7:104423–104434, 2019.
&lt;/div&gt;
&lt;div id=&#34;6&#34;&gt; 
[6] Peng Chu and Haibin Ling. Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking. In ICCV, 2019.
&lt;/div&gt;
&lt;div id =&#34;7&#34;&gt;
[7] Wenwei Zhang, Hui Zhou, Shuyang Sun, Zhe Wang, Jianping Shi, and Chen Change Loy. Robust multi-modality multi-object tracking. In ICCV, 2019.
&lt;/div&gt;
&lt;div id=&#34;8&#34;&gt;
[8] Michael D Breitenstein, Fabian Reichlin, Bastian Leibe, Esther Koller-Meier, and Luc Van Gool. Online multiperson tracking-by-detection from a single, uncalibrated camera. TPAMI, 33(9):1820–1833, 2010.
&lt;/div&gt;
&lt;div id=&#34;9&#34;&gt;
[9] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. Social lstm: Human trajectory prediction in crowded spaces. In CVPR, 2016.
&lt;/div&gt;
&lt;div id=&#34;10&#34;&gt;
[10] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajectories with generative adversarial networks. In CVPR, 2018.
&lt;/div&gt;
&lt;div id=&#34;11&#34;&gt;
[11] Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and Sergey Levine. Precog: Prediction conditioned on goals invisual multi-agent settings. In ICCV, 2019. 
&lt;/div&gt;
&lt;div id=&#34;12&#34;&gt;
[12] Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net. In CVPR, 2018.
&lt;/div&gt;
&lt;div id=13&gt;
[13] Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun. End-to-end interpretable neural motion planner. In CVPR, 2019.
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>A Novel Approach to Categorize News Articles From Headlines and Short Text</title>
      <link>https://abyaadrafid.github.io/publication/a-novel-approach-to-categorize-news-articles-from-headlines-and-short-text/</link>
      <pubDate>Sun, 25 Oct 2020 19:07:27 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/publication/a-novel-approach-to-categorize-news-articles-from-headlines-and-short-text/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Advantage Actor Critic</title>
      <link>https://abyaadrafid.github.io/project/a3c/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/a3c/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Deep-Reinforcement-Learning/blob/master/Policy%20Gradients/Advantage_Actor_Critic.ipynb&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Deep Q Learning</title>
      <link>https://abyaadrafid.github.io/project/dqn/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/dqn/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Deep-Reinforcement-Learning/blob/master/Deep%20Q%20Learning/Deep_Q_Learning.ipynb&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Double Deep Q Learning</title>
      <link>https://abyaadrafid.github.io/project/ddqn/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/ddqn/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Deep-Reinforcement-Learning/blob/master/Deep%20Q%20Learning/Double_Deep_Q_Learning.ipynb&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Dueling Deep Q Learning</title>
      <link>https://abyaadrafid.github.io/project/duellingdqn/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/duellingdqn/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Deep-Reinforcement-Learning/blob/master/Deep%20Q%20Learning/Dueling_Deep_Q_Learning.ipynb&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Dueling Double Deep Q Learning</title>
      <link>https://abyaadrafid.github.io/project/d3qn/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/d3qn/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Deep-Reinforcement-Learning/blob/master/Deep%20Q%20Learning/Dueling_Double_Deep_Q_Learning.ipynb&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Policy Gradient (REINFORCE)</title>
      <link>https://abyaadrafid.github.io/project/reinforce/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/reinforce/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Deep-Reinforcement-Learning/blob/master/Policy%20Gradients/Vanilla_Policy_Gradient%5BREINFORCE%5D.ipynb&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Soft Actor Critic</title>
      <link>https://abyaadrafid.github.io/project/sac/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/sac/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Deep-Reinforcement-Learning/blob/master/Policy%20Gradients/Soft_Actor_Critic.ipynb&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Value Actor Critic</title>
      <link>https://abyaadrafid.github.io/project/valueac/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/valueac/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Deep-Reinforcement-Learning/blob/master/Policy%20Gradients/Value_Actor_Critic.ipynb&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Aerial Cactus Identification</title>
      <link>https://abyaadrafid.github.io/project/acic/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/acic/</guid>
      <description>&lt;p&gt;This was one my of very first competitions. While doing &lt;a href=&#34;https://course.fast.ai/&#34;&gt;Practical Deep Learning for Coders&lt;/a&gt;, this competition provided a good source of practise. It was a binary classification problem. The goal for this competition was to determine whether the given satellite image contained a columnur cactus.&lt;/p&gt;
&lt;p&gt;I used this dataset for two purposes :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To implement and test &lt;a href=&#34;https://arxiv.org/pdf/1801.07698.pdf&#34;&gt;ArcFace&lt;/a&gt; using pytorch.&lt;/li&gt;
&lt;li&gt;To get placed into a high LeaderBoard position in the competition using FastAI.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;h3 id=&#34;eda&#34;&gt;EDA&lt;/h3&gt;
&lt;p&gt;According to the dataset details, the images were taken from the air. The images are low-res, some of them rotated to arbitrary angles and some zoomed. From visual inspection, the cacti are somewhat easy to spot because of their unique texture and stick-like shape. The class imbalance is not severe, can be handled by data augmentation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;data_print.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;data-split-and-transforms&#34;&gt;Data split and Transforms&lt;/h3&gt;
&lt;h4 id=&#34;split&#34;&gt;Split&lt;/h4&gt;
&lt;p&gt;As the class imbalance was not servere, the data could be split into train/valid set at random.&lt;/p&gt;
&lt;h4 id=&#34;transforms&#34;&gt;Transforms&lt;/h4&gt;
&lt;p&gt;Following Transforms were applied with 75% probability to augment the data, then the images were resized to 128*128. Test time augmentation was not applied.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Horizontal Flip&lt;/li&gt;
&lt;li&gt;Vertical Flip&lt;/li&gt;
&lt;li&gt;Left and Right rotation upto 10°&lt;/li&gt;
&lt;li&gt;Upto 110% zoom&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;hyperparameters&#34;&gt;Hyperparameters&lt;/h3&gt;
&lt;h4 id=&#34;arcface&#34;&gt;ArcFace&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;s = 64&lt;/li&gt;
&lt;li&gt;m = 0.0&lt;/li&gt;
&lt;li&gt;Adam Optimizer with fixed lr = 1e-3&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;competition-classifiers&#34;&gt;Competition Classifiers&lt;/h4&gt;
&lt;h5 id=&#34;densenet169&#34;&gt;Densenet169&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;Frozen model, Adam optimizer with maximum lr = 7.5e-3.&lt;/li&gt;
&lt;li&gt;CyclirLR scheduler&lt;/li&gt;
&lt;li&gt;Unfrozen model, Adam optimizer with maximum lr = 1e-6.&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;resnet101&#34;&gt;Resnet101&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;Frozen model, Adam optimizer with maximum lr = 9e-3.&lt;/li&gt;
&lt;li&gt;CyclirLR scheduler&lt;/li&gt;
&lt;li&gt;Unfrozen model, Adam optimizer with maximum lr = 1e-6.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;model-performance&#34;&gt;Model Performance&lt;/h3&gt;
&lt;p&gt;I used DenseNet169 and Resnet101 for Leaderboard and ArcFace for research purposes.&lt;/p&gt;
&lt;h3 id=&#34;arcface-1&#34;&gt;ArcFace&lt;/h3&gt;
&lt;p&gt;ArcFace was applied on the Resnet101 backbone. Implemented from scratch in pytorch. With embedding dimension = 2048 and scale_factor (s) = 64, accuracy follows :&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;arcface.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;Further experimentation using additional linear layers can boost the results. Then again, this approach is designed for one-shot learning. Worse performance in Binary Classification is quite understandable.&lt;/p&gt;
&lt;h4 id=&#34;densenet169-1&#34;&gt;DenseNet169&lt;/h4&gt;
&lt;p&gt;Densenet169 needs more time to converge because of its enormous size and paramters.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;epoch&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;train_loss&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;valid_loss&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;error_rate&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;accuracy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.059754&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.004154&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:35&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.062731&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000837&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.019187&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.003954&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.009922&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000457&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.004491&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000055&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:27&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;resnet101-1&#34;&gt;Resnet101&lt;/h4&gt;
&lt;p&gt;Resnet101 needed less time to converge.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;epoch&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;train_loss&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;valid_loss&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;error_rate&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;accuracy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.063169&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.033260&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.011429&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.988571&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.034835&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.002770&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.024171&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.002123&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.014281&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.006416&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.005714&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.994286&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.006923&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.002465&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01:13&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;competition-standings&#34;&gt;Competition Standings&lt;/h2&gt;
&lt;p&gt;My models acheived perfect accuracy score in the public leaderboard.&lt;/p&gt;
&lt;form action=&#34;https://github.com/abyaadrafid/Aerial-Cactus-Identification&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>APTOS Blindness Detection</title>
      <link>https://abyaadrafid.github.io/project/aptos/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/aptos/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/APTOS-Blindness-Detection&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Computer Vision 101</title>
      <link>https://abyaadrafid.github.io/project/f360/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/f360/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Fruits-360&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Facial Expression Recognition</title>
      <link>https://abyaadrafid.github.io/project/ferm/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/ferm/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Face-Expression-Recognition&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Quora Insincere Questions Classification</title>
      <link>https://abyaadrafid.github.io/project/qiqc/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/qiqc/</guid>
      <description>&lt;p&gt;The problem for this competition was text binary classification. The goal of this competition was to develop a model that can flag potentially toxic/insincere questions. Being a kernel-only competition, models available memory and execution time was limited for submissions. I developed a model and submitted it to the competition after it had ended. The submissions placed me at :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Public LB  : Top 50&lt;/li&gt;
&lt;li&gt;Private LB : Top 100&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;QuoraDiagram.jpeg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;aproach&#34;&gt;Aproach&lt;/h2&gt;
&lt;h3 id=&#34;eda&#34;&gt;EDA&lt;/h3&gt;
&lt;p&gt;Initial EDA of the data revealed the dataset to be very imbalanced. Around 96% of the data was from one class and the rest from the other. So accuracy as a metric was useless in this particular competition. The evaluation criteria was to maximized F-score, rightly so.&lt;/p&gt;
&lt;h3 id=&#34;text-preprocessing-and-loading&#34;&gt;Text Preprocessing and Loading&lt;/h3&gt;
&lt;p&gt;The competition provided word embedding files. To use the word embeddings, I needed to make sure that word coverage is as much as possible. That meant I needed to fix word misspellings and typos. Glove and Paragram word embeddings do not include many punctuations present in the dataset, so I needed to remove them as well.&lt;/p&gt;
&lt;h4 id=&#34;processing-steps&#34;&gt;Processing Steps&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Cleaning Text from numbers and Punctuations&lt;/li&gt;
&lt;li&gt;Replacing typical misspellings and typos&lt;/li&gt;
&lt;li&gt;Filling blank questions with special tokens&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;crafting-meta-features&#34;&gt;Crafting Meta Features&lt;/h4&gt;
&lt;p&gt;Based on some EDA, these features are extracted from the train data :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Total length&lt;/li&gt;
&lt;li&gt;Capitals&lt;/li&gt;
&lt;li&gt;Ratio of capitals and length&lt;/li&gt;
&lt;li&gt;Number of words&lt;/li&gt;
&lt;li&gt;Number of unique words&lt;/li&gt;
&lt;li&gt;Ratio of unique words and total words&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;tokenization&#34;&gt;Tokenization&lt;/h4&gt;
&lt;p&gt;I used keras tokenizer. All out of vocabulary words will be replaced with special &amp;lsquo;xxunk&amp;rsquo; token. Tokenizer is fit on the train data and used to make sequence of texts. Then the sequences are padded so all of them have the same length. Longer sequences are truncated and shorter ones are padded.&lt;/p&gt;
&lt;h4 id=&#34;scaling-statistical-features&#34;&gt;Scaling Statistical Features&lt;/h4&gt;
&lt;p&gt;Calculated Statistical Features will have to be scaled before putting through a neuralnet.&lt;/p&gt;
&lt;p&gt;As because these features have different ranges for values, some features may produce vanishing gradients. For example number of words will be, on average, a way larger value than number of unique words. So, these feaures are scaled using a standard scaler.&lt;/p&gt;
&lt;h3 id=&#34;loading-embeddings&#34;&gt;Loading Embeddings&lt;/h3&gt;
&lt;p&gt;I have used paragram and glove embeddings in the model. As these word embeddings were not trained in exactly the same system, some useful information may be lost if a weighted average is used. That is why, I have concatenated them into a num_words * (embedding_dim * num_embedding) matrix.&lt;/p&gt;
&lt;p&gt;Using more embeddings may end up giving more generalization but these models are quite memory intensive and used all my ram. So I couldn&#39;t use more than two of them.&lt;/p&gt;
&lt;h3 id=&#34;model-definition&#34;&gt;Model Definition&lt;/h3&gt;
&lt;h4 id=&#34;modules&#34;&gt;Modules&lt;/h4&gt;
&lt;p&gt;Following Modules contruct the model :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Embedding and Dropout&lt;/li&gt;
&lt;li&gt;LSTM+GRU backbone&lt;/li&gt;
&lt;li&gt;Attention Block&lt;/li&gt;
&lt;li&gt;Extractor/ Merge Layer&lt;/li&gt;
&lt;li&gt;Output/ Head&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;embedding-and-dropout&#34;&gt;Embedding and Dropout&lt;/h4&gt;
&lt;p&gt;This layer takes truncated sequence of tokenized texts as input and provides a 2d array as output.
In my case I limited 120000 as the maximum number of unique features to use. A dropout of 10% was applied. This module returns an array of size (120000 * 300 * 2)&lt;/p&gt;
&lt;h4 id=&#34;lstmgru-and-attention-backbone&#34;&gt;LSTM+GRU and Attention Backbone&lt;/h4&gt;
&lt;p&gt;The main body of the model consists of bidirectional LSTM and GRU (128).
The body returns LSTM+GRU outputs and Attention-transformed LSTM+GRU outputs.&lt;/p&gt;
&lt;h4 id=&#34;extractor-merge&#34;&gt;Extractor/ Merge&lt;/h4&gt;
&lt;p&gt;This layer serves as the feature extractor for lstm, gru outputs and statistical Meta features.
Conv1d is applied on lstm output, which goes into a maxpool. GRU output is sent through avgpool. Statistical Features are put through a Linear layer.&lt;/p&gt;
&lt;h4 id=&#34;output-layer&#34;&gt;Output Layer&lt;/h4&gt;
&lt;p&gt;This layer consists of a dropout, a relu and a batchnorm layer, sandwiched between two linear layers. The last linear layer provides the output of the model as a score for the input.&lt;/p&gt;
&lt;h3 id=&#34;complete-model&#34;&gt;Complete Model&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;First input is sent through embedding layer, returning feature vectors.&lt;/li&gt;
&lt;li&gt;Embedding output is then passed to the Body of the model.&lt;/li&gt;
&lt;li&gt;Extractor layer is used to find features from the sequential output of the model backbone.&lt;/li&gt;
&lt;li&gt;Attention infused lstm+gru outputs and extracted features are concatenated.&lt;/li&gt;
&lt;li&gt;Concatenated features are sent to the output layer.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;training-the-model&#34;&gt;Training the model&lt;/h3&gt;
&lt;p&gt;The model was trained 5 epochs with Stratified 5-fold-cross-validation. Adam optimizer with 0.01 learning rate and ExponentialLR scheduler was used.&lt;/p&gt;
&lt;h3 id=&#34;best-threshold-for-classification&#34;&gt;Best threshold for classification&lt;/h3&gt;
&lt;p&gt;The sigmoid value provided by the model is a continuous representation of insincerity in the inputs. We need a specific threshold to separate the two classes. The best threshold was calculated on the training data, then it was applied on the test data. Based on the threshold of the score, test data was classified and submitted.&lt;/p&gt;
&lt;form action=&#34;https://github.com/abyaadrafid/Quora-Insincere-Question-Classification&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Recursion Cellular Image Classification</title>
      <link>https://abyaadrafid.github.io/project/rcic/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/rcic/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Recursion-Cellular-Image-Classification&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>SIIM-ACR Pneumothorax Segmentation</title>
      <link>https://abyaadrafid.github.io/project/siim-acr/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/siim-acr/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/SIIM-ACR-Pneumothorax-Segmentation&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
    <item>
      <title>Tabular vs Vision Models</title>
      <link>https://abyaadrafid.github.io/project/dmnist/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0600</pubDate>
      <guid>https://abyaadrafid.github.io/project/dmnist/</guid>
      <description>&lt;form action=&#34;https://github.com/abyaadrafid/Digits-MNIST&#34;&gt;
    &lt;input type=&#34;submit&#34; formtarget = &#34;_blank&#34; value=&#34;Github Repo&#34; /&gt;
&lt;/form&gt;</description>
    </item>
    
  </channel>
</rss>
